﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿# Explanation for Problem 1: LRU Cache**Author:** Moya RichardsThis document provides a text explanation of the efficiency of the code and the design choices.------------------------## Project Summary|  | Summary Details || -------------- | --------------- | ---------------- || Efficiency | O(1) time complexity, and O(n) space complexity  || Data Structures | Map and Doubly linked list <br><br>(*In Python, the map concept appears as a built-in data type called a dictionary.)*|## Design Choices### Data Structures UsedThe project utilizes **2 data structures**:| Data Structure | Time Complexity | Space Complexity || -------------- | --------------- | ---------------- || Map aka Python's dictionary | O(1) | O(n) || Doubly linked list | O(1) | O(n) |This project is about removing the least recently used (LRU) data in the cache when it becomes full. In order to accomplish that, I needed a way to move the most recently used items to the top of the cache and delete the least recently used item from the bottom.Each time a new item**(node)**  is added to the cache it is moved to the top of the list and when an item  is retrieved from the cache it is removed from the list, then reinserted at the top of the list. **A Doubly linked list** offers the flexibility needed to order the items in the cache. With **A Doubly linked list**  one keep track of the previous node as well as the next node, and can also add/prepend to the top/head and remove from the bottom/tail of the list.The project requirement is that O(1) time is used to solve the project. This means that operations to add, delete and update the data in the cache should not involve traversing the list, since traversal would result in O(n) time.For example a linear traversal to delete an item takes O(n) time because we might have to search through all the items in the Doubly linked list in order to delete it when found. This is a naïve and problematic approach, so in order to achieve O(1) time we have to avoid traversal.To work around this traversal issue, we will utilize a map to optimize the speed at which the items are accessed. Therefore, every item added to the cache will also be added to a map. By saving a pointer to the item in a map it will be easy to delete the item no matter where it appears in the Doubly linked list. In the Doubly linked list deletion is done by orphaning the "to be deleted item" . orphaning is achieved by linking the previous and next nodes  of the "to be deleted item" to each other. The deletion is then finalized by removing the item from the map. Using a map to improve the time is great, but it does add a bit more complexity to the project by taking up O(n) space.In the Doubly linked list I also kept track of the tail of the list in order to make it easy to shift the data from the end/tail of the list. This took O(1) time. traversing the list to find the last item added(a recently used item) would have taken O(n) time.Because data in a cache is added and retrieved via keys the Node used inside the Doubly linked stores the data as a key-value pair. Storing the key in the node, makes it easy to find the appropriate item in the map, when the least recently used item is to be deleted.> **Note: **In this project I create the DoublyLinkedList as a separate class so that it can be clearly seen where a doubly linked list was being used and where a map was being used in order to build the cache. The cache class  is defined as a subclass to the DoublyLinkedList data structure.------------------------## Udacity Project Requirement> **Least Recently Used Cache**We have briefly discussed caching as part of a practice problem while studying hash maps.The lookup operation (i.e., get()) and put() / set() is supposed to be fast for a cache memory.While doing the get() operation, if the entry is found in the cache, it is known as a cache hit. If, however, the entry is not found, it is known as a cache miss.When designing a cache, we also place an upper bound on the size of the cache. If the cache is full and we want to add a new entry to the cache, we use some criteria to remove an element. After removing an element, we use the put() operation to insert the new element. The remove operation should also be fast.For our first problem, the goal will be to design a data structure known as a Least Recently Used (LRU) cache. An LRU cache is a type of cache in which we remove the least recently used entry when the cache memory reaches its limit. For the current problem, consider both get and set operations as an use operation.Your job is to use an appropriate data structure(s) to implement the cache.    In case of a cache hit, your get() operation should return the appropriate value.    In case of a cache miss, your get() should return -1.    While putting an element in the cache, your put() / set() operation must insert the element. If the cache is full, you must write code that removes the least recently used entry first and then insert the element.**All operations must take O(1) time.**For the current problem, you can consider the size of cache = 5.Here is some boiler plate code and some example test cases to get you started on this problem:```pythonclass LRU_Cache(object):    def __init__(self, capacity):        # Initialize class variables        pass    def get(self, key):        # Retrieve item from provided key. Return -1 if nonexistent.         pass    def set(self, key, value):        # Set the value if the key is not present in the cache. If the cache is at capacity remove the oldest item.         pass```**Code to work with  cache**```pythonour_cache = LRU_Cache(5)our_cache.set(1, 1);our_cache.set(2, 2);our_cache.set(3, 3);our_cache.set(4, 4);our_cache.get(1)       # returns 1our_cache.get(2)       # returns 2our_cache.get(9)      # returns -1 because 9 is not present in the cacheour_cache.set(5, 5) our_cache.set(6, 6)our_cache.get(3)      # returns -1 because the cache reached it's capacity and 3 was the least recently used entry```